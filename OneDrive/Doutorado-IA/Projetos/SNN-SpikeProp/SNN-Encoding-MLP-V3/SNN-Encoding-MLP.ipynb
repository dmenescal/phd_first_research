{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos parâmetros para treinamento da rede neural utilizando backpropagation:\n",
    "1) Taxa de aprendizagem para camada de Encoding: <b>ta_enc</b>\n",
    "2) Taxa de aprendizagem para camada de MLP: <b>ta_mlp</b>\n",
    "3) Pesos sinápticos na camada de Encoding [m][u][i][j]: <b>W_enc</b>\n",
    "4) Pesos sinápticos na camada de MLP [m][u][f][i][j] : <b>W_mlp</b>\n",
    "5) Base de dados de Imagens [m][n][n] : <b>Img_ds</b>\n",
    "6) Tempos de disparo na camada de Encoding [m][u][s] : <b>ST_enc</b>\n",
    "7) Tempos de disparo na camada de MLP [m][f][s] : <b>ST_mlp</b>\n",
    "8) Número máximo de épocas: <b>max_epochs</b>\n",
    "9) Erro mínimo: <b>Err_min</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição das características físicas do neurônio LIF (Leaky Integrate-and-Fire):\n",
    "1) Resistência geral da membrana: <b>lif_resistance</b>\n",
    "2) Capacitância geral da membrana: <b>lif_capacitance</b>\n",
    "3) Constante de tempo (RC) da membrana: <b>lif_tao</b>\n",
    "4) Potencial da membrana: <b>lif_voltage</b>\n",
    "5) Potencial de repouso da membrana: <b>lif_rest_voltage</b>\n",
    "6) Limiar de disparo de impulsos: <b>lif_threshold</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do método Forward-Pass da Rede Neural. Junto, o empacotamento de todos os parâmetros de entrada necessários em um formato de dicionário (chave-valor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' input_params = {\\n\"input_data\":image,\\n\"patch_size\": patch_size,\\n\"patch_stride\": stride,\\n\"lif_neuron_resistance\": R, \\n\"lif_neuron_capacitance\": C, \\n\"lif_neuron_time_constant\": R*C, \\n\"lif_neuron_rest_voltage\": lif_rest, \\n\"lif_encoding_threshold\": theta_enc, \\n\"lif_mlp_threshold\": theta_mlp, \\n\"num_samples\": num_samples, \\n\"epochs\": num_epochs, \\n\"min_err\": min_err, \\n\"num_classes\": num_classes, \\n\"encoding_weights\": W_enc, \\n\"mlp_weights\": W_mlp,\\n\"encoding_learning_rate\": eta_enc, \\n\"mlp_learning_rate\": eta_mlp,\\n\"time_window\": time_window\\n} '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" input_params = {\n",
    "\"input_data\":image,\n",
    "\"patch_size\": patch_size,\n",
    "\"patch_stride\": stride,\n",
    "\"lif_neuron_resistance\": R, \n",
    "\"lif_neuron_capacitance\": C, \n",
    "\"lif_neuron_time_constant\": R*C, \n",
    "\"lif_neuron_rest_voltage\": lif_rest, \n",
    "\"lif_encoding_threshold\": theta_enc, \n",
    "\"lif_mlp_threshold\": theta_mlp, \n",
    "\"num_samples\": num_samples, \n",
    "\"epochs\": num_epochs, \n",
    "\"min_err\": min_err, \n",
    "\"num_classes\": num_classes, \n",
    "\"encoding_weights\": W_enc, \n",
    "\"mlp_weights\": W_mlp,\n",
    "\"encoding_learning_rate\": eta_enc, \n",
    "\"mlp_learning_rate\": eta_mlp,\n",
    "\"time_window\": time_window\n",
    "} \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução em cadeia do processo de geração da saída do modelo da rede neural.\n",
    "# Nessa primeira versão, vamos utilizar uma amostra apenas como entrada.\n",
    "# O pipeline de execução (forward pass) possui os seguintes passos:\n",
    "## <b> 1. Geração dos patches a partir da amostra </b>\n",
    "## <b> 2. Criação da matriz de pesos sinápticos da camada de encoding </b>\n",
    "## <b> 3. Cálculo da Corrente sináptica da camada de encoding (elo sináptico) </b>\n",
    "## <b> 4. Geração dos impulsos de saída da camada de encoding (elo de ativação) </b>\n",
    "## <b> 5. Criação da matriz de pesos sinápticos da camada de mlp </b>\n",
    "## <b> 6. Cálculo da Corrente sináptica da camada de mlp (elo sináptico) </b>\n",
    "## <b> 7. Geração dos impulsos de saída da camada mlp de saída (elo de ativação) </b>\n",
    "## <b> 8. Geração do conjunto de valores esperados na saída do modelo </b>\n",
    "## <b> 9. Cálculo do sinal de erro </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de uma estrutura contendo os principais parâmetros.\n",
    "input_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 0: Seleção de uma amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe escolhida(dígito):5\n"
     ]
    }
   ],
   "source": [
    "# Apenas uma amostra aleatória. Escolher baseado na classe. Varia entre 0 e 9.\n",
    "sample_label = np.random.randint(0,9)\n",
    "\n",
    "# Obter amostras apenas daquela classe.\n",
    "samples = x_train[np.where(y_train == sample_label)]\n",
    "num_samples = samples.shape[0]\n",
    "\n",
    "# Filtrar apenas por uma amostra aleatória.\n",
    "sample_idx = np.random.randint(0, num_samples-1)\n",
    "image_sample = samples[sample_idx]\n",
    "\n",
    "# Guarda a amostra e sua classe em uma tupla\n",
    "sample_tuple = {\"sample\": image_sample, \"label\": sample_label}\n",
    "print(\"Classe escolhida(dígito):{}\".format(sample_tuple['label']))\n",
    "\n",
    "input_params['sample_set'] = image_sample\n",
    "input_params['sample_label_set'] = sample_label\n",
    "input_params['num_classes'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: Geração de Patches através da amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patches_extraction(image, stride, patch_size):\n",
    "    patches = []\n",
    "    num_patches = 0\n",
    "    ci = 0\n",
    "    n = image.shape[1]\n",
    "    while(ci < n):\n",
    "        cj = 0\n",
    "        while(cj < n):\n",
    "            pivot = (ci, cj)\n",
    "            if(pivot[0]+patch_size >= n or pivot[1]+patch_size >= n):\n",
    "                cj += stride\n",
    "            else:\n",
    "                patch = image[pivot[0]:pivot[0]+patch_size, pivot[1]:pivot[1]+patch_size]\n",
    "                patches.append(patch)\n",
    "                num_patches += 1\n",
    "                cj += stride\n",
    "        ci += stride\n",
    "    return np.asarray(patches), num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do conjunto de patches: (529, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "input_params['patch_size'] = 5\n",
    "input_params['patch_stride'] = 1\n",
    "patches, num_patches = patches_extraction(input_params['sample_set'], \n",
    "                             input_params['patch_stride'], \n",
    "                             input_params['patch_size'])\n",
    "\n",
    "print(\"Dimensão do conjunto de patches:\",patches.shape)\n",
    "input_params['patches_set'] = patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: Criação da matriz de pesos sinápticos da camada de encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os valores da matriz de pesos estão entre 0 e 1, com distribuição uniforme.\n",
    "# Dimensão da matriz: [num_patches][patch_size][patch_size]\n",
    "W_enc = np.random.uniform(0, 1, (input_params['patches_set'].shape))\n",
    "input_params['encoding_weights'] = W_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Geração da corrente sináptica da camada de encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_synaptic_current(patches, encoding_weights):\n",
    "    synaptic_current = np.zeros(patches.shape[0])\n",
    "    patch_size = patches.shape[1]\n",
    "    for p in range(patches.shape[0]):\n",
    "        for i in range(patch_size):\n",
    "            for j in range(patch_size):\n",
    "                synaptic_current[p] += patches[p][i][j]*encoding_weights[p][i][j]\n",
    "    return synaptic_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão da Corrente sináptica de encoding:  (529,)\n"
     ]
    }
   ],
   "source": [
    "I_enc = encoding_synaptic_current(input_params['patches_set'], \n",
    "                                  input_params['encoding_weights'])\n",
    "print(\"Dimensão da Corrente sináptica de encoding: \", I_enc.shape)\n",
    "input_params['encoding_layer_neurons_num'] = I_enc.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 4: Geração dos Impulsos de saída da camada de encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da dinâmica do neurônio LIF\n",
    "\n",
    "import math\n",
    "def lif_activation(R, I, V_rest, V0, tao, t):\n",
    "    return (V_rest+(R*I))-(V_rest+(R*I)+V0)*math.exp((-1*t)/(tao))\n",
    "\n",
    "\n",
    "def integrate_and_fire_spikes(lif_R, lif_tao, lif_rest_voltage, lif_start_voltage, lif_threshold, I_syn, time_duration):\n",
    "    spike_train = []\n",
    "    spike_time = []\n",
    "    spike_count = 0\n",
    "    for t in range(time_duration):\n",
    "        new_voltage = lif_activation(lif_R, I_syn, lif_rest_voltage, lif_start_voltage, lif_tao, t)\n",
    "        if (new_voltage >= lif_threshold):\n",
    "            spike_train.append(1)\n",
    "            spike_time.append(t)\n",
    "            spike_count += 1\n",
    "        else:\n",
    "            spike_train.append(0)\n",
    "            \n",
    "    return np.asarray(spike_train), np.asarray(spike_time), spike_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos Impulsos de saída da camada de Encoding: (529, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmene\\Anaconda3\\envs\\snn-pso\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros biológicos do neurônio LIF\n",
    "input_params['lif_enc_resistance'] = 1000000\n",
    "input_params['lif_enc_tao'] = 10\n",
    "input_params['lif_enc_threshold'] = -50\n",
    "input_params['lif_enc_rest_potential'] = -65\n",
    "input_params['lif_enc_start_potential'] = 0\n",
    "input_params['lif_simulation_time'] = 100\n",
    "\n",
    "encoding_spikes = []\n",
    "encoding_spike_time = []\n",
    "encoding_spike_count = []\n",
    "\n",
    "for neuron in range(input_params['encoding_layer_neurons_num']):\n",
    "    spikes, spike_times, spike_count = integrate_and_fire_spikes(lif_R=input_params['lif_enc_resistance'], \n",
    "                                                    lif_tao=input_params['lif_enc_tao'],\n",
    "                                                    lif_rest_voltage=input_params['lif_enc_rest_potential'], \n",
    "                                                    lif_start_voltage=input_params['lif_enc_start_potential'], \n",
    "                                                    lif_threshold=input_params['lif_enc_threshold'], \n",
    "                                                    I_syn=I_enc[neuron], \n",
    "                                                    time_duration=input_params['lif_simulation_time'])\n",
    "    encoding_spikes.append(spikes)\n",
    "    encoding_spike_time.append(spike_times)\n",
    "    encoding_spike_count.append(spike_count)\n",
    "\n",
    "encoding_spikes = np.asarray(encoding_spikes)\n",
    "encoding_spike_time = np.asarray(encoding_spike_time)\n",
    "encoding_spike_count = np.asarray(encoding_spike_count)\n",
    "input_params['encoding_spikes'] = encoding_spikes\n",
    "input_params['encoding_spike_times'] = encoding_spike_time\n",
    "input_params['encoding_spike_count'] = encoding_spike_count\n",
    "\n",
    "print(\"Dimensão dos Impulsos de saída da camada de Encoding:\", encoding_spikes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 5: Criação da matriz de pesos sinápticos da camada de mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os números gerados são aleatórios entre 0 e 1, através de uma distribuição uniforme.\n",
    "# A dimensão da matriz é [num_neuronios_enc][num_neuronios_mlp][tempo], onde num_neuronios_mlp = num_classes\n",
    "\n",
    "enc_neuron_num = input_params['encoding_layer_neurons_num']\n",
    "mlp_neuron_num = input_params['num_classes']\n",
    "t = input_params['lif_simulation_time']\n",
    "\n",
    "W_mlp = np.random.uniform(0, 1, (mlp_neuron_num, enc_neuron_num, t))\n",
    "input_params['mlp_weights'] = W_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 6: Cálculo da Corrente sináptica da camada de mlp (elo sináptico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_synaptic_current(encoding_spike_train, encoding_layer_size, mlp_layer_size, mlp_weights, time_duration):\n",
    "    synaptic_current = []\n",
    "    for neuron in range(mlp_layer_size):\n",
    "        current = 0\n",
    "        for u in range(encoding_layer_size):\n",
    "            for s in range(time_duration):\n",
    "                current += encoding_spike_train[u][s] * mlp_weights[neuron][u][s]\n",
    "        synaptic_current.append(current)\n",
    "    return np.asarray(synaptic_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão da Corrente sináptica de mlp:  (10,)\n"
     ]
    }
   ],
   "source": [
    "I_mlp = mlp_synaptic_current(encoding_spike_train=input_params['encoding_spikes'], \n",
    "                             encoding_layer_size=input_params['encoding_layer_neurons_num'], \n",
    "                             mlp_layer_size=input_params['num_classes'], \n",
    "                             mlp_weights=input_params['mlp_weights'],\n",
    "                             time_duration=input_params['lif_simulation_time'])\n",
    "print(\"Dimensão da Corrente sináptica de mlp: \", I_mlp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 7: Geração dos impulsos de saída da camada mlp de saída (elo de ativação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos Impulsos de saída da camada de MLP: (10, 100)\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros biológicos do neurônio LIF\n",
    "input_params['lif_mlp_resistance'] = 1000000\n",
    "input_params['lif_mlp_tao'] = 10\n",
    "input_params['lif_mlp_threshold'] = -50\n",
    "input_params['lif_mlp_rest_potential'] = -65\n",
    "input_params['lif_mlp_start_potential'] = 0\n",
    "\n",
    "mlp_spikes = []\n",
    "mlp_spike_times = []\n",
    "mlp_spike_count = []\n",
    "\n",
    "for neuron in range(input_params['num_classes']):\n",
    "    spikes, spike_times, spike_count = integrate_and_fire_spikes(lif_R=input_params['lif_mlp_resistance'], \n",
    "                                                    lif_tao=input_params['lif_mlp_tao'],\n",
    "                                                    lif_rest_voltage=input_params['lif_mlp_rest_potential'], \n",
    "                                                    lif_start_voltage=input_params['lif_mlp_start_potential'], \n",
    "                                                    lif_threshold=input_params['lif_mlp_threshold'], \n",
    "                                                    I_syn=I_enc[neuron], \n",
    "                                                    time_duration=input_params['lif_simulation_time'])\n",
    "    mlp_spikes.append(spikes)\n",
    "    mlp_spike_times.append(spike_times)\n",
    "    mlp_spike_count.append(spike_count)\n",
    "\n",
    "mlp_spikes = np.asarray(mlp_spikes)\n",
    "mlp_spike_times = np.asarray(mlp_spike_times)\n",
    "mlp_spike_count = np.asarray(mlp_spike_count)\n",
    "\n",
    "input_params['mlp_spikes'] = mlp_spikes\n",
    "input_params['mlp_spike_times'] = mlp_spike_times\n",
    "input_params['mlp_spike_count'] = mlp_spike_count\n",
    "\n",
    "print(\"Dimensão dos Impulsos de saída da camada de MLP:\", mlp_spikes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 8: Geração do conjunto de valores esperados na saída do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 9), (10, 19), (20, 29), (30, 39), (40, 49), (50, 59), (60, 69), (70, 79), (80, 89), (90, 99)]\n"
     ]
    }
   ],
   "source": [
    "# A geração dos spike trains esperados para a rede neural, a priori, não é definido por um conjunto de valores fixos.\n",
    "# Cada classe é representada por um array de 100 posições com valores inteiros, representando cada momento em que um spike foi gerado ou não.\n",
    "# Para não criar um viés na definição e deixar o mais \"randômico\" possível, para cada classe esperada (0 a 9), serão geradas \n",
    "# sequências de valores aleatórios dentro de intervalos definidos para cada classe:\n",
    "# Classe 0: Intervalo de valores inteiros de 0 a 9\n",
    "# Classe 1: Intervalo de valores inteiros de 10 a 19\n",
    "# Classe 2: Intervalo de valores inteiros de 20 a 29\n",
    "# Classe 3: Intervalo de valores inteiros de 30 a 39\n",
    "# Classe 4: Intervalo de valores inteiros de 40 a 49\n",
    "# Classe 5: Intervalo de valores inteiros de 50 a 59\n",
    "# Classe 6: Intervalo de valores inteiros de 60 a 69\n",
    "# Classe 7: Intervalo de valores inteiros de 70 a 79\n",
    "# Classe 8: Intervalo de valores inteiros de 80 a 89\n",
    "# Classe 9: Intervalo de valores inteiros de 90 a 99\n",
    "# Dessa forma a união dos intervalos vai corresponder ao intervalo completo de simulação, que no caso é de 100 passos (\"ms\")\n",
    "\n",
    "\n",
    "def interval_generator(number_of_classes=10, simulation_time=100):\n",
    "    interval = []\n",
    "    for i in range(number_of_classes):\n",
    "        interval.append((i*int((simulation_time/number_of_classes)),-1 + ((i+1)*int((simulation_time/number_of_classes)))))\n",
    "    return interval\n",
    "\n",
    "print(interval_generator()) # Geração Default de teste\n",
    "\n",
    "desired_spike_range_per_class = interval_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 5. 8. 8. 5. 4. 4. 6. 3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# Um segundo detalhe é a quantidade de spikes por classe. Como cada classe possui uma quantidade máxima fixa (tempo_simulacao/num_classes), podemos definir uma geração\n",
    "# aleatória na quantidade de spikes com limite máximo superior.\n",
    "\n",
    "def spike_count_generator(number_of_classes=10, simulation_time=100):\n",
    "    spike_counts_per_class = np.zeros(number_of_classes)\n",
    "    for i in range(number_of_classes):\n",
    "        spike_counts_per_class[i] = np.random.randint(0, number_of_classes)\n",
    "    return spike_counts_per_class\n",
    "\n",
    "desired_spike_count_per_class = spike_count_generator()\n",
    "\n",
    "print(desired_spike_count_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      "  0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def spike_times_generator(spike_range, spike_count):\n",
    "    desired_spike_times = []\n",
    "    desired_spike_times = np.zeros((input_params['num_classes'], input_params['lif_simulation_time']))\n",
    "    num_classes = input_params['num_classes']\n",
    "    for i in range(num_classes):\n",
    "        spike_time = random.sample(range(spike_range[i][0], spike_range[i][1]), int(spike_count[i]))\n",
    "        desired_spike_times[i][spike_time] = 1\n",
    "    return desired_spike_times\n",
    "\n",
    "desired_spike_times = spike_times_generator(desired_spike_range_per_class, desired_spike_count_per_class)\n",
    "\n",
    "input_params['desired_spikes'] = desired_spike_times\n",
    "\n",
    "print(desired_spike_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 9: Cálculo do sinal de erro do modelo da rede neural e energia instantânea do erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_energy_value(error_signal):\n",
    "    sqr_error = 0\n",
    "    for k in range(input_params['num_classes']):\n",
    "        for t in range(input_params['lif_simulation_time']):\n",
    "            sqr_error += error_signal[k][t]**2\n",
    "    return 0.5*sqr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1.,  0.,  0.,  0., -1., -1., -1.,  0., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,  0., -1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_signal = input_params['desired_spikes']-input_params['mlp_spikes']\n",
    "error_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "94.0\n"
     ]
    }
   ],
   "source": [
    "error_energy = error_energy_value(error_signal)\n",
    "print(error_energy.shape)\n",
    "print(error_energy)\n",
    "input_params['error_energy'] = error_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sample_set', 'sample_label_set', 'num_classes', 'patch_size', 'patch_stride', 'patches_set', 'encoding_weights', 'encoding_layer_neurons_num', 'lif_enc_resistance', 'lif_enc_tao', 'lif_enc_threshold', 'lif_enc_rest_potential', 'lif_enc_start_potential', 'lif_simulation_time', 'encoding_spikes', 'encoding_spike_times', 'encoding_spike_count', 'mlp_weights', 'lif_mlp_resistance', 'lif_mlp_tao', 'lif_mlp_threshold', 'lif_mlp_rest_potential', 'lif_mlp_start_potential', 'mlp_spikes', 'mlp_spike_times', 'mlp_spike_count', 'desired_spikes', 'error_energy'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesta etapa será realizado o processo de atualização dos pesos sinápticos por camada.\n",
    "# Para isso iremos dividir em 3 etapas:\n",
    "# 1) Cálculo do gradiente da camada de MLP\n",
    "# 2) Cálculo do gradiente da camada de Encoding\n",
    "# 3) Cálculo da Regra Delta generalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 0: Derivada do elo de ativação. O elo de ativação corresponde a função de ativação dos neurônios LIF, que por sua vez é representado pelo potencial elétrico da membrana. A derivada calculada deve ser em função do campo local induzido, ou seja, da corrente sináptica de entrada do neurônio. Como o elo de ativação também possui a dimensão de tempo devido a dinâmica do potencial, a derivada calculada é avaliada exatamente no tempo onde ocorreu um disparo de impulso. Na composição do gradiente, o valor do sinal de erro traz o tempo de disparo 's', logo esse deve ser o mesmo tempo a ser utilizado no cálculo da derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_activation_derivative(R, I, V_rest, V0, tao, t):\n",
    "    return ((V_rest+(R*I)+V0)*math.exp((-1*t)/(tao)))/tao                                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: Gradiente local da camada de MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_local_gradient(sample_error_signal):\n",
    "    local_gradient = np.zeros((input_params['num_classes'], input_params['lif_simulation_time']))\n",
    "    for k in range(input_params['num_classes']):\n",
    "        for t in range(input_params['lif_simulation_time']):\n",
    "            local_gradient[k][t] = (-1)*sample_error_signal[k][t]*lif_activation_derivative(R     = input_params['lif_mlp_resistance'],\n",
    "                                                                                            I     = I_mlp[k], \n",
    "                                                                                            V_rest= input_params['lif_mlp_rest_potential'], \n",
    "                                                                                            V0    = input_params['lif_mlp_start_potential'], \n",
    "                                                                                            tao   = input_params['lif_mlp_tao'], \n",
    "                                                                                            t     = t)\n",
    "    return local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "mlp_local_gradients = mlp_local_gradient(error_signal)\n",
    "print(mlp_local_gradients.shape)\n",
    "input_params['mlp_local_gradient'] = mlp_local_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: Gradiente local da camada de Encoding. Aqui, cada neurônio precida dos gradientes calculados anteriormente na camada de MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_local_gradient(mlp_gradients, mlp_weights):\n",
    "    local_gradient = np.zeros((input_params['encoding_layer_neurons_num']))\n",
    "    for e in range(input_params['encoding_layer_neurons_num']):\n",
    "        acc = 0\n",
    "        for k in range(input_params['num_classes']):\n",
    "            for t in range(input_params['lif_simulation_time']):\n",
    "                acc += mlp_gradients[k][t]*mlp_weights[k][e][t]\n",
    "        local_gradient[e] = acc*lif_activation_derivative(R      = input_params['lif_enc_resistance'],\n",
    "                                                          I      = I_enc[e], \n",
    "                                                          V_rest = input_params['lif_enc_rest_potential'], \n",
    "                                                          V0     = input_params['lif_enc_start_potential'], \n",
    "                                                          tao    = input_params['lif_enc_tao'], \n",
    "                                                          t      = t)\n",
    "    return local_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(529, 100)\n"
     ]
    }
   ],
   "source": [
    "enc_local_gradients = enc_local_gradient(input_params['mlp_local_gradient'], input_params['mlp_weights'])\n",
    "input_params['enc_local_gradient'] = enc_local_gradients\n",
    "print(enc_local_gradients.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Regra Delta aplicado para cada gradiente local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do learning rate para a rede neural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params['snn_learning_rate'] = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3.1: Regra Delta aplicado para a camada de Encoding. Nesse caso, como os pesos sinápticos da camada de encoding não possuem relação temporal, ao contrário da camada mlp, é necessário carregar um outro tipo de informação sobre os impulsos gerados. Foi escolhido o spike count, que representa um vetor temporal em um escalar. O resultado final é que o valor do Delta é um escalar, permitindo que os ajustes dos pesos ocorra através de uma soma \"element-wise\" do peso anterior com um valor escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_rule_encoding(learning_rate, encoding_gradient, encoding_output):\n",
    "    delta = []\n",
    "    for d in range(input_params['encoding_layer_neurons_num']):\n",
    "        delta.append(learning_rate*encoding_gradient[d]*encoding_output[d])\n",
    "    return np.asarray(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3.2: Regra Delta aplicado para a camada MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_rule(learning_rate, local_gradient, output):\n",
    "    return learning_rate*local_gradient*output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Revisão das Dimensões para o cálculo da regra delta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos pesos da camada mlp: (10, 529, 100)\n",
      "Dimensão do gradiente da camada mlp: (10, 100)\n",
      "Dimensão do spike train da camada mlp: (10, 100)\n",
      "Dimensão dos pesos da camada encoding: (529, 5, 5)\n",
      "Dimensão do gradiente da camada encoding: (529, 100)\n",
      "Dimensão do spike train da camada encoding: (529, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensão dos pesos da camada mlp:',input_params['mlp_weights'].shape)\n",
    "print('Dimensão do gradiente da camada mlp:',input_params['mlp_local_gradient'].shape)\n",
    "print('Dimensão do spike train da camada mlp:',input_params['mlp_spikes'].shape)\n",
    "print('Dimensão dos pesos da camada encoding:',input_params['encoding_weights'].shape)\n",
    "print('Dimensão do gradiente da camada encoding:',input_params['enc_local_gradient'].shape)\n",
    "print('Dimensão do spike train da camada encoding:',input_params['encoding_spikes'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do DeltaW da camada de encoding: (529,)\n",
      "Dimensão do DeltaW da camada de mlp: (10, 100)\n"
     ]
    }
   ],
   "source": [
    "delta_W_mlp = delta_rule(learning_rate=input_params['snn_learning_rate'],\n",
    "                         local_gradient=input_params['mlp_local_gradient'],\n",
    "                         output=input_params['mlp_spikes'])\n",
    "\n",
    "delta_W_enc = delta_rule_encoding(learning_rate=input_params['snn_learning_rate'],\n",
    "                         encoding_gradient=input_params['enc_local_gradient_old'],\n",
    "                         encoding_output=input_params['encoding_spike_count'])\n",
    "\n",
    "input_params['delta_W_enc'] = delta_W_enc\n",
    "input_params['delta_W_mlp'] = delta_W_mlp\n",
    "\n",
    "print('Dimensão do DeltaW da camada de encoding:', delta_W_enc.shape)\n",
    "print('Dimensão do DeltaW da camada de mlp:', delta_W_mlp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação dos novos pesos sinápticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_mlp_weights(mlp_weights, delta):\n",
    "    new_weights = []\n",
    "    for d in range(input_params['encoding_layer_neurons_num']):\n",
    "        new_weights.append(mlp_weights[:, d]+delta)\n",
    "    return np.reshape(np.asarray(new_weights), (input_params['num_classes'], input_params['encoding_layer_neurons_num'], input_params['lif_simulation_time']))\n",
    "\n",
    "def compute_new_enc_weights(enc_weights, delta):\n",
    "    new_weights = []\n",
    "    for d in range(input_params['encoding_layer_neurons_num']):\n",
    "        new_weights.append(enc_weights[d]+delta[d])\n",
    "    return np.asarray(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos pesos da camada de encoding: (529, 5, 5)\n",
      "Dimensão dos pesos da camada de mlp: (10, 529, 100)\n"
     ]
    }
   ],
   "source": [
    "new_mlp_weights = compute_new_mlp_weights(input_params['mlp_weights'], input_params['delta_W_mlp'])\n",
    "new_enc_weights = compute_new_enc_weights(input_params['encoding_weights'], input_params['delta_W_enc'])\n",
    "\n",
    "print('Dimensão dos pesos da camada de encoding:', new_enc_weights.shape)\n",
    "print('Dimensão dos pesos da camada de mlp:', new_mlp_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e43dd8bb75c62a6ecf1af3f36a5e1d06e418643f35b874ce51f57c1f90762855"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('snn-pso': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
